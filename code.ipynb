{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_MZr5fWt_9U",
        "outputId": "c3f75c7d-b196-4dd5-84a4-2cd941d17591"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy==1.23.4 in /usr/local/lib/python3.10/dist-packages (1.23.4)\n"
          ]
        }
      ],
      "source": [
        "pip install numpy==1.23.4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyworld"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgY8goETxcIr",
        "outputId": "f1e9c8d0-a549-41d2-9bb1-7b15a0fc0f9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyworld\n",
            "  Using cached pyworld-0.3.3.tar.gz (218 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cython>=0.24 in /usr/local/lib/python3.10/dist-packages (from pyworld) (0.29.34)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pyworld) (1.23.4)\n",
            "Building wheels for collected packages: pyworld\n",
            "  Building wheel for pyworld (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyworld: filename=pyworld-0.3.3-cp310-cp310-linux_x86_64.whl size=887714 sha256=49e40377dd8a82606d56e8b756175b85abcdaa69c4dd32fccd5cd50164bc82d2\n",
            "  Stored in directory: /root/.cache/pip/wheels/70/50/a9/36b47c7f055bbee666a2b5718aaf85bce2152ef90f9bd10697\n",
            "Successfully built pyworld\n",
            "Installing collected packages: pyworld\n",
            "Successfully installed pyworld-0.3.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "from urllib.request import urlretrieve\n",
        "import zipfile\n",
        "import argparse\n",
        "import shlex, subprocess\n",
        "import zipfile\n",
        "\n",
        "\n",
        "def unzip(zip_filepath, dest_dir='./data'):\n",
        "    with zipfile.ZipFile(zip_filepath) as zf:\n",
        "        zf.extractall(dest_dir)\n",
        "    print(\"Extraction complete!\")\n",
        "\n",
        "def download_vcc2016():\n",
        "    datalink=\"https://datashare.is.ed.ac.uk/bitstream/handle/10283/2211/\"\n",
        "    data_files = ['vcc2016_training.zip', 'evaluation_all.zip']\n",
        "\n",
        "    if os.path.exists(data_files[0]) or os.path.exists(data_files[1]):\n",
        "        print(\"File already exists!\")\n",
        "        return\n",
        "\n",
        "    trainset = f'{datalink}/{data_files[0]}'\n",
        "    evalset = f'{datalink}/{data_files[1]}'\n",
        "\n",
        "    train_comm = f'wget {trainset}'\n",
        "    eval_comm = f'wget {evalset}'\n",
        "\n",
        "    train_comm = shlex.split(train_comm)\n",
        "    eval_comm = shlex.split(eval_comm)\n",
        "\n",
        "    print('Start download dataset...')\n",
        "    \n",
        "    subprocess.run(train_comm)\n",
        "    subprocess.run(eval_comm)\n",
        "\n",
        "    unzip(data_files[0])\n",
        "    unzip(data_files[1])\n",
        "    \n",
        "    print('Finish download dataset...')\n",
        "\n",
        "def create_dirs(trainset: str='./data/speakers', testset: str='./data/speakers_test'):\n",
        "    '''create train test dirs'''\n",
        "    if not os.path.exists(trainset):\n",
        "        print(f'create train set dir {trainset}')\n",
        "        os.makedirs(trainset, exist_ok=True)\n",
        "\n",
        "    if not os.path.exists(testset):\n",
        "        print(f'create test set dir {testset}')\n",
        "        os.makedirs(testset, exist_ok=True)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    parser = argparse.ArgumentParser(description = 'Download  voice conversion datasets.')\n",
        "\n",
        "    datasets_default = 'vcc2016'\n",
        "    train_dir = './data/speakers'\n",
        "    test_dir = './data/speakers_test'\n",
        "    parser.add_argument('--datasets', type = str, help = 'Datasets available: vcc2016', default = datasets_default)\n",
        "    \n",
        "    parser.add_argument('--train_dir', type = str, help = 'trainset directory', default = train_dir)\n",
        "    parser.add_argument('--test_dir', type = str, help = 'testset directory', default = test_dir)\n",
        "\n",
        "    argv = parser.parse_args(\"\")\n",
        "\n",
        "    datasets = argv.datasets\n",
        "    create_dirs(train_dir, test_dir)\n",
        "\n",
        "    if datasets == 'vcc2016' or datasets == 'VCC2016':\n",
        "        download_vcc2016()\n",
        "    else:\n",
        "        print('Dataset not available.')\n",
        "\n",
        "   "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J611Z1dwxmxY",
        "outputId": "ecfb5c57-cfd7-48e8-c836-dea0ca426bdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File already exists!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pyworld as pw\n",
        "import os,shutil\n",
        "import glob\n",
        "import librosa\n",
        "\n",
        "class Singleton(type):\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        self.__instance = None\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        if self.__instance is None:\n",
        "            self.__instance = super().__call__(*args, **kwargs)\n",
        "            return self.__instance\n",
        "        else:\n",
        "            return self.__instance\n",
        "\n",
        "class CommonInfo(metaclass=Singleton):\n",
        "    \"\"\"docstring for CommonInfo.\"\"\"\n",
        "    def __init__(self, datadir: str):\n",
        "        super(CommonInfo, self).__init__()\n",
        "        self.datadir = datadir\n",
        "    \n",
        "    @property\n",
        "    def speakers(self):\n",
        "        ''' return current selected speakers for training\n",
        "        eg. ['SF2', 'TM1', 'SF1', 'TM2']\n",
        "        '''\n",
        "        p = os.path.join(self.datadir, \"*\")\n",
        "        all_sub_folder = glob.glob(p)\n",
        "            \n",
        "        all_speaker = [s.rsplit('/', maxsplit=1)[1] for s in all_sub_folder]\n",
        "        all_speaker.sort()\n",
        "        return all_speaker\n",
        "\n",
        "speakers = CommonInfo('data/speakers').speakers\n",
        "\n",
        "\n",
        "class Normalizer(object):\n",
        "    '''Normalizer: convience method for fetch normalize instance'''\n",
        "    def __init__(self, statfolderpath: str='./etc'):\n",
        "        \n",
        "        self.folderpath = statfolderpath\n",
        "\n",
        "        self.norm_dict = self.normalizer_dict()\n",
        "\n",
        "    def forward_process(self, x, speakername):\n",
        "        mean = self.norm_dict[speakername]['coded_sps_mean']\n",
        "        std = self.norm_dict[speakername]['coded_sps_std']\n",
        "        mean = np.reshape(mean, [-1,1])\n",
        "        std = np.reshape(std, [-1,1])\n",
        "        x = (x - mean) / std\n",
        "\n",
        "        return x\n",
        "\n",
        "    def backward_process(self, x, speakername):\n",
        "        mean = self.norm_dict[speakername]['coded_sps_mean']\n",
        "        std = self.norm_dict[speakername]['coded_sps_std']\n",
        "        mean = np.reshape(mean, [-1,1])\n",
        "        std = np.reshape(std, [-1,1])\n",
        "        x = x * std + mean\n",
        "\n",
        "        return x\n",
        "\n",
        "    def normalizer_dict(self):\n",
        "        '''return all speakers normailzer parameter'''\n",
        "\n",
        "        d = {}\n",
        "        for one_speaker in speakers:\n",
        "\n",
        "            p = os.path.join(self.folderpath, '*.npz')\n",
        "            try:\n",
        "                stat_filepath = [fn for fn in glob.glob(p) if one_speaker in fn][0]\n",
        "            except:\n",
        "                raise Exception('====no match files!====')\n",
        "            # print(f'[load]: {stat_filepath}')\n",
        "            t = np.load(stat_filepath)\n",
        "            d[one_speaker] = t\n",
        "\n",
        "        return d\n",
        "    \n",
        "    def pitch_conversion(self, f0, source_speaker, target_speaker):\n",
        "        '''Logarithm Gaussian normalization for Pitch Conversions'''\n",
        "        \n",
        "        mean_log_src = self.norm_dict[source_speaker]['log_f0s_mean']\n",
        "        std_log_src = self.norm_dict[source_speaker]['log_f0s_std']\n",
        "\n",
        "        mean_log_target = self.norm_dict[target_speaker]['log_f0s_mean']\n",
        "        std_log_target = self.norm_dict[target_speaker]['log_f0s_std']\n",
        "\n",
        "        f0_converted = np.exp((np.ma.log(f0) - mean_log_src) / std_log_src * std_log_target + mean_log_target)\n",
        "\n",
        "        return f0_converted\n",
        "    \n",
        "class GenerateStatistics(object):\n",
        "    def __init__(self, folder: str ='./data/processed'):\n",
        "        self.folder = folder\n",
        "        self.include_dict_npz = {}\n",
        "        for s in speakers:\n",
        "            if not self.include_dict_npz.__contains__(s):\n",
        "                self.include_dict_npz[s] = []\n",
        "\n",
        "            for one_file in os.listdir(folder):\n",
        "                if one_file.startswith(s) and one_file.endswith('npz'):\n",
        "                    self.include_dict_npz[s].append(one_file)\n",
        "        \n",
        "\n",
        "    @staticmethod\n",
        "    def coded_sp_statistics(coded_sps):\n",
        "        # sp shape (D, T)\n",
        "        coded_sps_concatenated = np.concatenate(coded_sps, axis = 1)\n",
        "        coded_sps_mean = np.mean(coded_sps_concatenated, axis = 1, keepdims = False)\n",
        "        coded_sps_std = np.std(coded_sps_concatenated, axis = 1, keepdims = False)\n",
        "        return coded_sps_mean, coded_sps_std\n",
        "\n",
        "    @staticmethod\n",
        "    def logf0_statistics(f0s):\n",
        "        log_f0s_concatenated = np.ma.log(np.concatenate(f0s))\n",
        "        log_f0s_mean = log_f0s_concatenated.mean()\n",
        "        log_f0s_std = log_f0s_concatenated.std()\n",
        "\n",
        "        return log_f0s_mean, log_f0s_std\n",
        "\n",
        "    def generate_stats(self, statfolder: str = 'etc'):\n",
        "        '''generate all user's statistics used for calutate normalized\n",
        "           input like sp, f0\n",
        "           step 1: generate coded_sp mean std\n",
        "           step 2: generate f0 mean std\n",
        "         '''\n",
        "        etc_path = os.path.join(os.path.realpath('.'), statfolder)\n",
        "        os.makedirs(etc_path, exist_ok=True)\n",
        "\n",
        "        for one_speaker in self.include_dict_npz.keys():\n",
        "            f0s = []\n",
        "            coded_sps = []           \n",
        "            arr01 = self.include_dict_npz[one_speaker]\n",
        "            if len(arr01) == 0:\n",
        "                continue\n",
        "            for one_file in arr01:\n",
        "                t = np.load(os.path.join(self.folder, one_file))\n",
        "                f0_ = np.reshape(t['f0'], [-1,1])\n",
        "\n",
        "                f0s.append(f0_)\n",
        "                coded_sps.append(t['coded_sp'])\n",
        "            \n",
        "            log_f0s_mean, log_f0s_std = self.logf0_statistics(f0s)\n",
        "            coded_sps_mean, coded_sps_std = self.coded_sp_statistics(coded_sps)\n",
        "\n",
        "            print(f'log_f0s_mean:{log_f0s_mean} log_f0s_std:{log_f0s_std}')\n",
        "            print(f'coded_sps_mean:{coded_sps_mean.shape}  coded_sps_std:{coded_sps_std.shape}')\n",
        "\n",
        "            filename = os.path.join(etc_path, f'{one_speaker}-stats.npz')\n",
        "            np.savez(filename, \n",
        "            log_f0s_mean=log_f0s_mean, log_f0s_std=log_f0s_std,\n",
        "            coded_sps_mean=coded_sps_mean, coded_sps_std=coded_sps_std)\n",
        "\n",
        "            print(f'[save]: {filename}')\n",
        "\n",
        "  \n",
        "    def normalize_dataset(self):\n",
        "        '''normalize dataset run once!'''\n",
        "        norm  = Normalizer()\n",
        "        files = librosa.util.find_files(self.folder, ext='npy')\n",
        "\n",
        "        for p in files:\n",
        "            filename = os.path.basename(p)\n",
        "            speaker = filename.split(sep='_', maxsplit=1)[0]\n",
        "            mcep = np.load(p)\n",
        "            mcep_normed = norm.forward_process(mcep, speaker)\n",
        "            os.remove(p)\n",
        "            np.save(p, mcep_normed)\n",
        "            print(f'[normalize]:{p}')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    pass"
      ],
      "metadata": {
        "id": "kRognQT-xvzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "import os\n",
        "import pyworld\n",
        "import pyworld as pw\n",
        "import glob\n",
        "#from utility import *\n",
        "import argparse\n",
        "from datetime import datetime\n",
        "\n",
        "FEATURE_DIM = 36\n",
        "SAMPLE_RATE = 16000\n",
        "FRAMES = 512\n",
        "FFTSIZE = 1024\n",
        "SPEAKERS_NUM = len(speakers)\n",
        "CHUNK_SIZE = 1 # concate CHUNK_SIZE audio clips together\n",
        "EPSILON = 1e-10\n",
        "MODEL_NAME = 'starganvc_model'\n",
        "\n",
        "def load_wavs(dataset: str, sr):\n",
        "    '''\n",
        "    data dict contains all audios file path &\n",
        "    resdict contains all wav files   \n",
        "    '''\n",
        "    data = {}\n",
        "    with os.scandir(dataset) as it:\n",
        "        for entry in it:\n",
        "            if entry.is_dir():\n",
        "                data[entry.name] = []\n",
        "                # print(entry.name, entry.path)\n",
        "                with os.scandir(entry.path) as it_f:\n",
        "                    for onefile in it_f:\n",
        "                        if onefile.is_file():\n",
        "                            # print(onefile.path)\n",
        "                            data[entry.name].append(onefile.path)\n",
        "    print(f'loaded keys: {data.keys()}')\n",
        "    #data like {TM1:[xx,xx,xxx,xxx]}\n",
        "    resdict = {}\n",
        "\n",
        "    cnt = 0\n",
        "    for key, value in data.items():\n",
        "        resdict[key] = {}\n",
        "\n",
        "        for one_file in value:\n",
        "            \n",
        "            filename = one_file.split('/')[-1].split('.')[0] #like 100061\n",
        "            newkey = f'{filename}'\n",
        "            wav, _ = librosa.load(one_file, sr=sr, mono=True, dtype=np.float64)\n",
        "            y,_ = librosa.effects.trim(wav, top_db=15)\n",
        "            wav = np.append(y[0], y[1:] - 0.97 * y[:-1])\n",
        "\n",
        "            resdict[key][newkey] = wav\n",
        "            # resdict[key].append(temp_dict) #like TM1:{100062:[xxxxx], .... }\n",
        "            print('.', end='')\n",
        "            cnt += 1\n",
        "\n",
        "    print(f'\\nTotal {cnt} aduio files!')\n",
        "    return resdict\n",
        "\n",
        "def chunks(iterable, size):\n",
        "    \"\"\"Yield successive n-sized chunks from iterable.\"\"\"\n",
        "    for i in range(0, len(iterable), size):\n",
        "        yield iterable[i:i + size]\n",
        "\n",
        "def wav_to_mcep_file(dataset: str, sr=SAMPLE_RATE, processed_filepath: str = './data/processed'):\n",
        "    '''convert wavs to mcep feature using image repr'''\n",
        "    shutil.rmtree(processed_filepath)\n",
        "    os.makedirs(processed_filepath, exist_ok=True)\n",
        "\n",
        "    allwavs_cnt = len(glob.glob(f'{dataset}/*/*.wav'))\n",
        "    print(f'Total {allwavs_cnt} audio files!')\n",
        "\n",
        "    d = load_wavs(dataset, sr)\n",
        "    for one_speaker in d.keys():\n",
        "        values_of_one_speaker = list(d[one_speaker].values())\n",
        "       \n",
        "        for index, one_chunk in enumerate (chunks(values_of_one_speaker, CHUNK_SIZE)):\n",
        "            wav_concated = [] #preserve one batch of wavs\n",
        "            temp = one_chunk.copy()\n",
        "\n",
        "            #concate wavs\n",
        "            for one in temp:\n",
        "                wav_concated.extend(one)\n",
        "            wav_concated = np.array(wav_concated)\n",
        "\n",
        "            #process one batch of wavs \n",
        "            f0, ap, sp, coded_sp = cal_mcep(wav_concated, sr=sr, dim=FEATURE_DIM)\n",
        "            newname = f'{one_speaker}_{index}'\n",
        "            file_path_z = os.path.join(processed_filepath, newname)\n",
        "            np.savez(file_path_z, f0=f0, coded_sp=coded_sp)\n",
        "            print(f'[save]: {file_path_z}')\n",
        "\n",
        "            #split mcep t0 muliti files  \n",
        "            for start_idx in range(0, coded_sp.shape[1] - FRAMES + 1, FRAMES):\n",
        "                one_audio_seg = coded_sp[:, start_idx : start_idx+FRAMES]\n",
        "\n",
        "                if one_audio_seg.shape[1] == FRAMES:\n",
        "                    temp_name = f'{newname}_{start_idx}'\n",
        "                    filePath = os.path.join(processed_filepath, temp_name)\n",
        "\n",
        "                    np.save(filePath, one_audio_seg)\n",
        "                    print(f'[save]: {filePath}.npy')\n",
        "            \n",
        "        \n",
        "\n",
        "def world_features(wav, sr, fft_size, dim):\n",
        "    f0, timeaxis = pyworld.harvest(wav, sr)\n",
        "    sp = pyworld.cheaptrick(wav, f0, timeaxis, sr,fft_size=fft_size)\n",
        "    ap = pyworld.d4c(wav, f0, timeaxis, sr, fft_size=fft_size)\n",
        "    coded_sp = pyworld.code_spectral_envelope(sp, sr, dim)\n",
        "\n",
        "    return f0, timeaxis, sp, ap, coded_sp\n",
        "\n",
        "def cal_mcep(wav, sr=SAMPLE_RATE, dim=FEATURE_DIM, fft_size=FFTSIZE):\n",
        "    '''cal mcep given wav singnal\n",
        "        the frame_period used only for pad_wav_to_get_fixed_frames\n",
        "    '''\n",
        "    f0, timeaxis, sp, ap, coded_sp = world_features(wav, sr, fft_size, dim)\n",
        "    coded_sp = coded_sp.T # dim x n\n",
        "\n",
        "    return f0, ap, sp, coded_sp\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    start = datetime.now()\n",
        "    parser = argparse.ArgumentParser(description = 'Convert the wav waveform to mel-cepstral coefficients(MCCs)\\\n",
        "    and calculate the speech statistical characteristics')\n",
        "    \n",
        "    input_dir = './data/speakers'\n",
        "    output_dir = './data/processed'\n",
        "   \n",
        "    parser.add_argument('--input_dir', type = str, help = 'the direcotry contains data need to be processed', default = input_dir)\n",
        "    parser.add_argument('--output_dir', type = str, help = 'the directory stores the processed data', default = output_dir)\n",
        "    \n",
        "    argv = parser.parse_args(\"\")\n",
        "    input_dir = argv.input_dir\n",
        "    output_dir = argv.output_dir\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    wav_to_mcep_file(input_dir, SAMPLE_RATE,  processed_filepath=output_dir)\n",
        "\n",
        "    #input_dir is train dataset. we need to calculate and save the speech\\\n",
        "    # statistical characteristics for each speaker.\n",
        "    generator = GenerateStatistics(output_dir)\n",
        "    generator.generate_stats()\n",
        "    generator.normalize_dataset()\n",
        "    end = datetime.now()\n",
        "    print(f\"[Runing Time]: {end-start}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTfgxAOZyI8N",
        "outputId": "2a19ff4c-343e-491c-a527-1b713b9783a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total 0 audio files!\n",
            "loaded keys: dict_keys([])\n",
            "\n",
            "Total 0 aduio files!\n",
            "[Runing Time]: 0:00:00.121582\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os\n",
        "\n",
        "import librosa\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "\n",
        "#from preprocess import (FEATURE_DIM, FFTSIZE, FRAMES, SAMPLE_RATE,\n",
        "#                        world_features)\n",
        "#from utility import Normalizer, speakers\n",
        "import random\n",
        "\n",
        "class AudioDataset(Dataset):\n",
        "    \"\"\"docstring for AudioDataset.\"\"\"\n",
        "    def __init__(self, datadir:str):\n",
        "        super(AudioDataset, self).__init__()\n",
        "        self.datadir = datadir\n",
        "        self.files = librosa.util.find_files(datadir, ext='npy')\n",
        "        self.encoder = LabelBinarizer().fit(speakers)\n",
        "        \n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        p = self.files[idx]\n",
        "        filename = os.path.basename(p)\n",
        "        speaker = filename.split(sep='_', maxsplit=1)[0]\n",
        "        label = self.encoder.transform([speaker])[0]\n",
        "        mcep = np.load(p)\n",
        "        mcep = torch.FloatTensor(mcep)\n",
        "        mcep = torch.unsqueeze(mcep, 0)\n",
        "        return mcep, torch.tensor(speakers.index(speaker), dtype=torch.long), torch.FloatTensor(label)\n",
        "\n",
        "    def speaker_encoder(self):\n",
        "        return self.encoder\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "def data_loader(datadir: str, batch_size=4, shuffle=True, mode='train', num_workers=2):\n",
        "    '''if mode is train datadir should contains training set which are all npy files\n",
        "        or, mode is test and datadir should contains only wav files.\n",
        "    '''\n",
        "    dataset = AudioDataset(datadir)\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n",
        "    \n",
        "    return loader\n",
        "\n",
        "\n",
        "\n",
        "class TestSet(object):\n",
        "    \"\"\"docstring for TestSet.\"\"\"\n",
        "    def __init__(self, datadir:str):\n",
        "        super(TestSet, self).__init__()\n",
        "        self.datadir = datadir\n",
        "        self.norm = Normalizer()\n",
        "        \n",
        "    def choose(self):\n",
        "        '''choose one speaker for test'''\n",
        "        r = random.choice(speakers)\n",
        "        return r\n",
        "    \n",
        "    def test_data(self, src_speaker=None):\n",
        "        '''choose one speaker for conversion'''\n",
        "        if src_speaker:\n",
        "            r_s = src_speaker\n",
        "        else:\n",
        "            r_s = self.choose()\n",
        "        p = os.path.join(self.datadir, r_s)\n",
        "        wavfiles = librosa.util.find_files(p, ext='wav')\n",
        "       \n",
        "        res = {}\n",
        "        for f in wavfiles:\n",
        "            filename = os.path.basename(f)\n",
        "            wav, _ = librosa.load(f, sr=SAMPLE_RATE, dtype=np.float64)\n",
        "            f0, timeaxis, sp, ap, coded_sp = world_features(wav, SAMPLE_RATE, FFTSIZE, FEATURE_DIM)\n",
        "            coded_sp_norm = self.norm.forward_process(coded_sp.T, r_s)\n",
        "\n",
        "            if not res.__contains__(filename):\n",
        "                res[filename] = {}\n",
        "            res[filename]['coded_sp_norm'] = np.asarray(coded_sp_norm)\n",
        "            res[filename]['f0'] = np.asarray(f0)\n",
        "            res[filename]['ap'] = np.asarray(ap)\n",
        "        return res , r_s    \n",
        "\n",
        "if __name__=='__main__':\n",
        "\n",
        "    # t = TestSet('data/speakers_test')\n",
        "    # # mcep, f0, speaker = t[0]\n",
        "    # # print(speaker)\n",
        "    # # print(mcep)\n",
        "    # # print(f0)\n",
        "    # # print(np.ma.log(f0))\n",
        "    # d, speaker = t.test_data()\n",
        "    \n",
        "\n",
        "    # for filename, content in d.items():\n",
        "    #     coded_sp_norm = content['coded_sp_norm']\n",
        "    #     print(content['coded_sp_norm'].shape)\n",
        "    #     f_len = coded_sp_norm.shape[1]\n",
        "    #     if  f_len >= FRAMES: \n",
        "    #         pad_length = FRAMES-(f_len - (f_len//FRAMES) * FRAMES)\n",
        "    #     elif f_len < FRAMES:\n",
        "    #         pad_length = FRAMES - f_len\n",
        "        \n",
        "    #     coded_sp_norm = np.hstack((coded_sp_norm, np.zeros((coded_sp_norm.shape[0], pad_length))))\n",
        "    #     print('after:' , coded_sp_norm.shape)\n",
        "    # print(t[1])\n",
        "    ad = AudioDataset('./data/processed')\n",
        "    print(len(ad))\n",
        "\n",
        "    data, s,label = ad[367]\n",
        "    print(data, label) \n",
        "    # loader = data_loader('./data/processed', batch_size=4)   \n",
        "    \n",
        "    # for i_batch, batch_data in enumerate(loader):\n",
        "    #     # print(batch_data)\n",
        "    #     # print(batch_data[0])\n",
        "    #     print(batch_data[1])\n",
        "    #     print(batch_data[2])\n",
        "    #     if i_batch == 2:\n",
        "    #         break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "SnFsUjKoyKcE",
        "outputId": "fbba4210-c4b8-4d63-9ee5-f41c66666d47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-2227563528aa>\u001b[0m in \u001b[0;36m<cell line: 87>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;31m#     print('after:' , coded_sp_norm.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;31m# print(t[1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0mad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAudioDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/processed'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-2227563528aa>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, datadir)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatadir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatadir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatadir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelBinarizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspeakers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    306\u001b[0m             )\n\u001b[1;32m    307\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y has 0 samples: %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_input_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: y has 0 samples: []"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "#from data_loader import data_loader\n",
        "#from utility import speakers\n",
        "\n",
        "\n",
        "class Down2d(nn.Module):\n",
        "    \"\"\"docstring for Down2d.\"\"\"\n",
        "    def __init__(self, in_channel ,out_channel, kernel, stride, padding):\n",
        "        super(Down2d, self).__init__()\n",
        "\n",
        "        self.c1 = nn.Conv2d(in_channel, out_channel, kernel_size=kernel, stride=stride, padding=padding)\n",
        "        self.n1 = nn.InstanceNorm2d(out_channel)\n",
        "        self.c2 = nn.Conv2d(in_channel, out_channel, kernel_size=kernel, stride=stride, padding=padding)\n",
        "        self.n2 = nn.InstanceNorm2d(out_channel)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x1 = self.c1(x)\n",
        "        x1 = self.n1(x1)\n",
        "\n",
        "        x2 = self.c2(x)\n",
        "        x2 = self.n2(x2)\n",
        "\n",
        "        x3 =  x1 * torch.sigmoid(x2)\n",
        "        \n",
        "        return x3\n",
        "            \n",
        "        \n",
        "class Up2d(nn.Module):\n",
        "    \"\"\"docstring for Up2d.\"\"\"\n",
        "    def __init__(self, in_channel ,out_channel, kernel, stride, padding):\n",
        "        super(Up2d, self).__init__()\n",
        "        self.c1 = nn.ConvTranspose2d(in_channel, out_channel, kernel_size=kernel, stride=stride, padding=padding)\n",
        "        self.n1 = nn.InstanceNorm2d(out_channel)\n",
        "        self.c2 = nn.ConvTranspose2d(in_channel, out_channel, kernel_size=kernel, stride=stride, padding=padding)\n",
        "        self.n2 = nn.InstanceNorm2d(out_channel)\n",
        "         \n",
        "    def forward(self, x):\n",
        "        x1 = self.c1(x)\n",
        "        x1 = self.n1(x1)\n",
        "\n",
        "        x2 = self.c2(x)\n",
        "        x2 = self.n2(x2)\n",
        "\n",
        "        x3 =  x1 * torch.sigmoid(x2)\n",
        "        \n",
        "        return x3\n",
        "        \n",
        "\n",
        "class Generator(nn.Module):\n",
        "    \"\"\"docstring for Generator.\"\"\"\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.downsample = nn.Sequential(\n",
        "            Down2d(1, 32, (3,9), (1,1), (1,4)),\n",
        "            Down2d(32, 64, (4,8), (2,2), (1,3)),\n",
        "            Down2d(64, 128, (4,8), (2,2), (1,3)),\n",
        "            Down2d(128, 64, (3,5), (1,1), (1,2)),\n",
        "            Down2d(64, 5, (9,5), (9,1), (1,2))\n",
        "        )\n",
        "\n",
        "        \n",
        "        self.up1 = Up2d(9, 64, (9,5), (9,1), (0,2))\n",
        "        self.up2 = Up2d(68, 128, (3,5), (1,1), (1,2))\n",
        "        self.up3 = Up2d(132, 64, (4,8), (2,2), (1,3))\n",
        "        self.up4 = Up2d(68, 32, (4,8), (2,2), (1,3))\n",
        "\n",
        "        self.deconv = nn.ConvTranspose2d(36, 1, (3,9), (1,1), (1,4))\n",
        "\n",
        "    def forward(self, x, c):\n",
        "        x = self.downsample(x)\n",
        "        c = c.view(c.size(0), c.size(1), 1, 1)\n",
        "\n",
        "        c1 = c.repeat(1, 1, x.size(2), x.size(3))\n",
        "        x = torch.cat([x, c1], dim=1)\n",
        "        x = self.up1(x)\n",
        "\n",
        "        c2 = c.repeat(1,1,x.size(2), x.size(3))\n",
        "        x = torch.cat([x, c2], dim=1)\n",
        "        x = self.up2(x)\n",
        "\n",
        "        c3 = c.repeat(1,1,x.size(2), x.size(3))\n",
        "        x = torch.cat([x, c3], dim=1)\n",
        "        x = self.up3(x)\n",
        "\n",
        "        c4 = c.repeat(1,1,x.size(2), x.size(3))\n",
        "        x = torch.cat([x, c4], dim=1)\n",
        "        x = self.up4(x)\n",
        "\n",
        "        c5 = c.repeat(1,1, x.size(2), x.size(3))\n",
        "        x = torch.cat([x, c5], dim=1)\n",
        "        x = self.deconv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    \"\"\"docstring for Discriminator.\"\"\"\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        \n",
        "        self.d1 = Down2d(5, 32, (3,9), (1,1), (1,4))\n",
        "        self.d2 = Down2d(36, 32, (3,8), (1,2), (1,3))    \n",
        "        self.d3 = Down2d(36, 32, (3,8), (1,2), (1,3))    \n",
        "        self.d4 = Down2d(36, 32, (3,6), (1,2), (1,2)) \n",
        "        \n",
        "        self.conv = nn.Conv2d(36, 1, (36,5), (36,1), (0,2))\n",
        "        self.pool = nn.AvgPool2d((1,64))\n",
        "    def forward(self, x, c):\n",
        "        c = c.view(c.size(0), c.size(1), 1, 1)\n",
        "        \n",
        "        c1 = c.repeat(1, 1, x.size(2), x.size(3))\n",
        "        x = torch.cat([x, c1], dim=1)\n",
        "        x = self.d1(x)\n",
        "\n",
        "        c2 = c.repeat(1, 1, x.size(2), x.size(3))\n",
        "        x = torch.cat([x, c2], dim=1)\n",
        "        x = self.d2(x)\n",
        "\n",
        "        c3 = c.repeat(1, 1, x.size(2), x.size(3))\n",
        "        x = torch.cat([x, c3], dim=1)\n",
        "        x = self.d3(x)\n",
        "\n",
        "        c4 = c.repeat(1, 1, x.size(2), x.size(3))\n",
        "        x = torch.cat([x, c4], dim=1)\n",
        "        x = self.d4(x)\n",
        "\n",
        "        c5 = c.repeat(1, 1, x.size(2), x.size(3))\n",
        "        x = torch.cat([x, c5], dim=1)\n",
        "        x = self.conv(x)\n",
        "       \n",
        "        x = self.pool(x)\n",
        "        x = torch.squeeze(x)\n",
        "        x = torch.tanh(x)\n",
        "        return x\n",
        "\n",
        "class DomainClassifier(nn.Module):\n",
        "    \"\"\"docstring for DomainClassifier.\"\"\"\n",
        "    def __init__(self):\n",
        "        super(DomainClassifier, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            Down2d(1, 8, (4,4), (2,2), (5,1)),\n",
        "            Down2d(8, 16, (4,4), (2,2), (1,1)),\n",
        "            Down2d(16, 32, (4,4), (2,2), (0,1)),\n",
        "            Down2d(32, 16, (3,4), (1,2), (1,1)),\n",
        "            nn.Conv2d(16, 4, (1,4), (1,2), (0,1)),\n",
        "            nn.AvgPool2d((1,16)),\n",
        "            nn.LogSoftmax()\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "       x = x[:, :, 0:8, :]\n",
        "       x = self.main(x)\n",
        "       x = x.view(x.size(0), x.size(1))\n",
        "       return x\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "    # train_loader = data_loader('data/processed', 1)\n",
        "    # data_iter = iter(train_loader)\n",
        "\n",
        "\n",
        "    t = torch.rand([1,1,36,512])\n",
        "    l = torch.FloatTensor([[0,1,0,0]])\n",
        "    print(l.size())\n",
        "    # d1 = Down2d(1, 32, 1,1,1)\n",
        "    # print(d1(t).shape)\n",
        "\n",
        "    # u1 = Up2d(1,32,1,2,1)\n",
        "    # print(u1(t).shape)\n",
        "    # G = Generator()\n",
        "    # o1 = G(t, l)\n",
        "    # print(o1.shape)\n",
        "    \n",
        "    D = Discriminator()\n",
        "    o2 = D(t, l)\n",
        "    print(o2.shape, o2)\n",
        "\n",
        "    # C = DomainClassifier()\n",
        "    # o3 = C(t)\n",
        "    # print(o3.shape)\n",
        "    # m = nn.Softmax()\n",
        "    # input = torch.Tensor([[1,2],[5,5]])\n",
        "    # output = m(input)\n",
        "    # print(output)"
      ],
      "metadata": {
        "id": "R580hu4F0xtq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorboardX"
      ],
      "metadata": {
        "id": "ZV5ok8-u00CX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "class Logger(object):\n",
        "    \"\"\"Tensorboard logger.\"\"\"\n",
        "\n",
        "    def __init__(self, log_dir):\n",
        "        \"\"\"Initialize summary writer.\"\"\"\n",
        "        self.writer = tf.summary.create_file_writer(log_dir)\n",
        "\n",
        "    #def scalar_summary(self, tag, value, step):\n",
        "     #   \"\"\"Add scalar summary.\"\"\"\n",
        "      #  summary = tf.compat.v1.Summary(value=[tf.compat.v1.Summary.Value(tag=tag, simple_value=value)])\n",
        "       # self.writer.summary(summary, step)\n",
        "    \n",
        "    def scalar_summary(self, tag, value, step):\n",
        "        #\"\"\"Add scalar summary.\"\"\"\n",
        "      with self.writer.as_default():\n",
        "        tf.summary.scalar(tag, value, step=step)\n"
      ],
      "metadata": {
        "id": "vHzv3kHo02Ky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "import soundfile as sf\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "#from data_loader import TestSet\n",
        "#from model import Discriminator, DomainClassifier, Generator\n",
        "#from utility import Normalizer, speakers\n",
        "#from preprocess import FRAMES, SAMPLE_RATE, FFTSIZE\n",
        "import random\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from pyworld import decode_spectral_envelope, synthesize\n",
        "import librosa\n",
        "import ast\n",
        "\n",
        "\n",
        "class Solver(object):\n",
        "    \"\"\"docstring for Solver.\"\"\"\n",
        "    def __init__(self, data_loader, config):\n",
        "        \n",
        "        self.config = config\n",
        "        self.data_loader = data_loader\n",
        "        # Model configurations.\n",
        "        \n",
        "        self.lambda_cycle = config.lambda_cycle\n",
        "        self.lambda_cls = config.lambda_cls\n",
        "        self.lambda_identity = config.lambda_identity\n",
        "\n",
        "        # Training configurations.\n",
        "        self.data_dir = config.data_dir\n",
        "        self.test_dir = config.test_dir\n",
        "        self.batch_size = config.batch_size\n",
        "        self.num_iters = config.num_iters\n",
        "        self.num_iters_decay = config.num_iters_decay\n",
        "        self.g_lr = config.g_lr\n",
        "        self.d_lr = config.d_lr\n",
        "        self.c_lr = config.c_lr\n",
        "        self.n_critic = config.n_critic\n",
        "        self.beta1 = config.beta1\n",
        "        self.beta2 = config.beta2\n",
        "        self.resume_iters = config.resume_iters\n",
        "        \n",
        "\n",
        "        # Test configurations.\n",
        "        self.test_iters = config.test_iters\n",
        "        self.trg_speaker = ast.literal_eval(config.trg_speaker)\n",
        "        self.src_speaker = config.src_speaker\n",
        "\n",
        "        # Miscellaneous.\n",
        "        self.use_tensorboard = config.use_tensorboard\n",
        "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "        self.spk_enc = LabelBinarizer().fit(speakers)\n",
        "        # Directories.\n",
        "        self.log_dir = config.log_dir\n",
        "        self.sample_dir = config.sample_dir\n",
        "        self.model_save_dir = config.model_save_dir\n",
        "        self.result_dir = config.result_dir\n",
        "\n",
        "        # Step size.\n",
        "        self.log_step = config.log_step\n",
        "        self.sample_step = config.sample_step\n",
        "        self.model_save_step = config.model_save_step\n",
        "        self.lr_update_step = config.lr_update_step\n",
        "\n",
        "        # Build the model and tensorboard.\n",
        "        self.build_model()\n",
        "        if self.use_tensorboard:\n",
        "            self.build_tensorboard()\n",
        "    \n",
        "    def build_model(self):\n",
        "        self.G = Generator()\n",
        "        self.D = Discriminator()\n",
        "        self.C = DomainClassifier()\n",
        "\n",
        "        self.g_optimizer = torch.optim.Adam(self.G.parameters(), self.g_lr, [self.beta1, self.beta2])\n",
        "        self.d_optimizer = torch.optim.Adam(self.D.parameters(), self.d_lr, [self.beta1, self.beta2])\n",
        "        self.c_optimizer = torch.optim.Adam(self.C.parameters(), self.c_lr,[self.beta1, self.beta2])\n",
        "        \n",
        "        self.print_network(self.G, 'G')\n",
        "        self.print_network(self.D, 'D')\n",
        "        self.print_network(self.C, 'C')\n",
        "            \n",
        "        self.G.to(self.device)\n",
        "        self.D.to(self.device)\n",
        "        self.C.to(self.device)\n",
        "    \n",
        "    def print_network(self, model, name):\n",
        "        \"\"\"Print out the network information.\"\"\"\n",
        "        num_params = 0\n",
        "        for p in model.parameters():\n",
        "            num_params += p.numel()\n",
        "        print(model)\n",
        "        print(name)\n",
        "        print(\"The number of parameters: {}\".format(num_params))\n",
        "\n",
        "    def build_tensorboard(self):\n",
        "        \"\"\"Build a tensorboard logger.\"\"\"\n",
        "        #from logger import Logger\n",
        "        self.logger = Logger(self.log_dir)\n",
        "\n",
        "    def update_lr(self, g_lr, d_lr, c_lr):\n",
        "        \"\"\"Decay learning rates of the generator and discriminator and classifier.\"\"\"\n",
        "        for param_group in self.g_optimizer.param_groups:\n",
        "            param_group['lr'] = g_lr\n",
        "        for param_group in self.d_optimizer.param_groups:\n",
        "            param_group['lr'] = d_lr\n",
        "        for param_group in self.c_optimizer.param_groups:\n",
        "            param_group['lr'] = c_lr\n",
        "\n",
        "    def train(self):\n",
        "        # Learning rate cache for decaying.\n",
        "        g_lr = self.g_lr\n",
        "        d_lr = self.d_lr\n",
        "        c_lr = self.c_lr\n",
        "\n",
        "        start_iters = 0\n",
        "        if self.resume_iters:\n",
        "            pass\n",
        "        \n",
        "        norm = Normalizer()\n",
        "        data_iter = iter(self.data_loader)\n",
        "\n",
        "        print('Start training......')\n",
        "        start_time = datetime.now()\n",
        "\n",
        "        for i in range(start_iters, self.num_iters):\n",
        "            # =================================================================================== #\n",
        "            #                             1. Preprocess input data                                #\n",
        "            # =================================================================================== #\n",
        "             # Fetch real images and labels.\n",
        "            try:\n",
        "                x_real, speaker_idx_org, label_org = next(data_iter)\n",
        "            except:\n",
        "                data_iter = iter(self.data_loader)\n",
        "                x_real, speaker_idx_org, label_org = next(data_iter)           \n",
        "\n",
        "            # Generate target domain labels randomly.\n",
        "            rand_idx = torch.randperm(label_org.size(0))\n",
        "            label_trg = label_org[rand_idx]\n",
        "            speaker_idx_trg = speaker_idx_org[rand_idx]\n",
        "            \n",
        "            x_real = x_real.to(self.device)           # Input images.\n",
        "            label_org = label_org.to(self.device)     # Original domain one-hot labels.\n",
        "            label_trg = label_trg.to(self.device)     # Target domain one-hot labels.\n",
        "            speaker_idx_org = speaker_idx_org.to(self.device) # Original domain labels\n",
        "            speaker_idx_trg = speaker_idx_trg.to(self.device) #Target domain labels\n",
        "\n",
        "            # =================================================================================== #\n",
        "            #                             2. Train the discriminator                              #\n",
        "            # =================================================================================== #\n",
        "            # Compute loss with real audio frame.\n",
        "            CELoss = nn.CrossEntropyLoss()\n",
        "            cls_real = self.C(x_real)\n",
        "            cls_loss_real = CELoss(input=cls_real, target=speaker_idx_org)\n",
        "\n",
        "            self.reset_grad()\n",
        "            cls_loss_real.backward()\n",
        "            self.c_optimizer.step()\n",
        "             # Logging.\n",
        "            loss = {}\n",
        "            loss['C/C_loss'] = cls_loss_real.item()\n",
        "\n",
        "            out_r = self.D(x_real, label_org)\n",
        "            # Compute loss with fake audio frame.\n",
        "            x_fake = self.G(x_real, label_trg)\n",
        "            out_f = self.D(x_fake.detach(), label_trg)\n",
        "            d_loss_t = F.binary_cross_entropy_with_logits(input=out_f,target=torch.zeros_like(out_f, dtype=torch.float)) + \\\n",
        "                F.binary_cross_entropy_with_logits(input=out_r, target=torch.ones_like(out_r, dtype=torch.float))\n",
        "           \n",
        "            out_cls = self.C(x_fake)\n",
        "            d_loss_cls = CELoss(input=out_cls, target=speaker_idx_trg)\n",
        "\n",
        "            # Compute loss for gradient penalty.\n",
        "            alpha = torch.rand(x_real.size(0), 1, 1, 1).to(self.device)\n",
        "            x_hat = (alpha * x_real.data + (1 - alpha) * x_fake.data).requires_grad_(True)\n",
        "            out_src = self.D(x_hat, label_trg)\n",
        "            d_loss_gp = self.gradient_penalty(out_src, x_hat)\n",
        "\n",
        "            d_loss = d_loss_t + self.lambda_cls * d_loss_cls + 5*d_loss_gp\n",
        "\n",
        "            self.reset_grad()\n",
        "            d_loss.backward()\n",
        "            self.d_optimizer.step()\n",
        "\n",
        "\n",
        "            # loss['D/d_loss_t'] = d_loss_t.item()\n",
        "            # loss['D/loss_cls'] = d_loss_cls.item()\n",
        "            # loss['D/D_gp'] = d_loss_gp.item()\n",
        "            loss['D/D_loss'] = d_loss.item()\n",
        "\n",
        "            # =================================================================================== #\n",
        "            #                               3. Train the generator                                #\n",
        "            # =================================================================================== #        \n",
        "            if (i+1) % self.n_critic == 0:\n",
        "                # Original-to-target domain.\n",
        "                x_fake = self.G(x_real, label_trg)\n",
        "                g_out_src = self.D(x_fake, label_trg)\n",
        "                g_loss_fake = F.binary_cross_entropy_with_logits(input=g_out_src, target=torch.ones_like(g_out_src, dtype=torch.float))\n",
        "                \n",
        "                out_cls = self.C(x_real)\n",
        "                g_loss_cls = CELoss(input=out_cls, target=speaker_idx_org)\n",
        "\n",
        "                # Target-to-original domain.\n",
        "                x_reconst = self.G(x_fake, label_org)\n",
        "                g_loss_rec = F.l1_loss(x_reconst, x_real )\n",
        "\n",
        "                # Original-to-Original domain(identity).\n",
        "                x_fake_iden = self.G(x_real, label_org)\n",
        "                id_loss = F.l1_loss(x_fake_iden, x_real )\n",
        "\n",
        "                # Backward and optimize.\n",
        "                g_loss = g_loss_fake + self.lambda_cycle * g_loss_rec +\\\n",
        "                 self.lambda_cls * g_loss_cls + self.lambda_identity * id_loss\n",
        "                 \n",
        "                self.reset_grad()\n",
        "                g_loss.backward()\n",
        "                self.g_optimizer.step()\n",
        "\n",
        "                # Logging.\n",
        "                loss['G/loss_fake'] = g_loss_fake.item()\n",
        "                loss['G/loss_rec'] = g_loss_rec.item()\n",
        "                loss['G/loss_cls'] = g_loss_cls.item()\n",
        "                loss['G/loss_id'] = id_loss.item()\n",
        "                loss['G/g_loss'] = g_loss.item()\n",
        "            # =================================================================================== #\n",
        "            #                                 4. Miscellaneous                                    #\n",
        "            # =================================================================================== #\n",
        "            # Print out training information.\n",
        "            if (i+1) % self.log_step == 0:\n",
        "                et = datetime.now() - start_time\n",
        "                et = str(et)[:-7]\n",
        "                log = \"Elapsed [{}], Iteration [{}/{}]\".format(et, i+1, self.num_iters)\n",
        "                for tag, value in loss.items():\n",
        "                    log += \", {}: {:.4f}\".format(tag, value)\n",
        "                print(log)\n",
        "\n",
        "                if self.use_tensorboard:\n",
        "                    for tag, value in loss.items():\n",
        "                        self.logger.scalar_summary(tag, value, i+1)\n",
        "\n",
        "            # Translate fixed images for debugging.\n",
        "            if (i+1) % self.sample_step == 0:\n",
        "                with torch.no_grad():\n",
        "                    d, speaker = TestSet(self.test_dir).test_data()\n",
        "                    target = random.choice([x for x in speakers if x != speaker])\n",
        "                    label_t = self.spk_enc.transform([target])[0]\n",
        "                    label_t = np.asarray([label_t])\n",
        "\n",
        "                    for filename, content in d.items():\n",
        "                        f0 = content['f0']\n",
        "                        ap = content['ap']\n",
        "                        sp_norm_pad = self.pad_coded_sp(content['coded_sp_norm'])\n",
        "                        \n",
        "                        convert_result = []\n",
        "                        for start_idx in range(0, sp_norm_pad.shape[1] - FRAMES + 1, FRAMES):\n",
        "                            one_seg = sp_norm_pad[:, start_idx : start_idx+FRAMES]\n",
        "                            \n",
        "                            one_seg = torch.FloatTensor(one_seg).to(self.device)\n",
        "                            one_seg = one_seg.view(1,1,one_seg.size(0), one_seg.size(1))\n",
        "                            l = torch.FloatTensor(label_t)\n",
        "                            one_seg = one_seg.to(self.device)\n",
        "                            l = l.to(self.device)\n",
        "                            one_set_return = self.G(one_seg, l).data.cpu().numpy()\n",
        "                            one_set_return = np.squeeze(one_set_return)\n",
        "                            one_set_return = norm.backward_process(one_set_return, target)\n",
        "                            convert_result.append(one_set_return)\n",
        "\n",
        "                        convert_con = np.concatenate(convert_result, axis=1)\n",
        "                        convert_con = convert_con[:, 0:content['coded_sp_norm'].shape[1]]\n",
        "                        contigu = np.ascontiguousarray(convert_con.T, dtype=np.float64)   \n",
        "                        decoded_sp = decode_spectral_envelope(contigu, SAMPLE_RATE, fft_size=FFTSIZE)\n",
        "                        f0_converted = norm.pitch_conversion(f0, speaker, target)\n",
        "                        wav = synthesize(f0_converted, decoded_sp, ap, SAMPLE_RATE)\n",
        "\n",
        "                        name = f'{speaker}-{target}_iter{i+1}_{filename}'\n",
        "                        path = os.path.join(self.sample_dir, name)\n",
        "                        print(f'[save]:{path}')\n",
        "                        #librosa.output.write_wav(path, wav, SAMPLE_RATE)\n",
        "                        sf.write(path, wav, SAMPLE_RATE)\n",
        "                        \n",
        "            # Save model checkpoints.\n",
        "            if (i+1) % self.model_save_step == 0:\n",
        "                G_path = os.path.join(self.model_save_dir, '{}-G.ckpt'.format(i+1))\n",
        "                D_path = os.path.join(self.model_save_dir, '{}-D.ckpt'.format(i+1))\n",
        "                C_path = os.path.join(self.model_save_dir, '{}-C.ckpt'.format(i+1))\n",
        "                torch.save(self.G.state_dict(), G_path)\n",
        "                torch.save(self.D.state_dict(), D_path)\n",
        "                torch.save(self.C.state_dict(), C_path)\n",
        "                print('Saved model checkpoints into {}...'.format(self.model_save_dir))\n",
        "\n",
        "            # Decay learning rates.\n",
        "            if (i+1) % self.lr_update_step == 0 and (i+1) > (self.num_iters - self.num_iters_decay):\n",
        "                g_lr -= (self.g_lr / float(self.num_iters_decay))\n",
        "                d_lr -= (self.d_lr / float(self.num_iters_decay))\n",
        "                c_lr -= (self.c_lr / float(self.num_iters_decay))\n",
        "                self.update_lr(g_lr, d_lr, c_lr)\n",
        "                print ('Decayed learning rates, g_lr: {}, d_lr: {}.'.format(g_lr, d_lr))\n",
        "\n",
        "    def gradient_penalty(self, y, x):\n",
        "        \"\"\"Compute gradient penalty: (L2_norm(dy/dx) - 1)**2.\"\"\"\n",
        "        weight = torch.ones(y.size()).to(self.device)\n",
        "        dydx = torch.autograd.grad(outputs=y,\n",
        "                                   inputs=x,\n",
        "                                   grad_outputs=weight,\n",
        "                                   retain_graph=True,\n",
        "                                   create_graph=True,\n",
        "                                   only_inputs=True)[0]\n",
        "\n",
        "        dydx = dydx.view(dydx.size(0), -1)\n",
        "        dydx_l2norm = torch.sqrt(torch.sum(dydx**2, dim=1))\n",
        "        return torch.mean((dydx_l2norm-1)**2)\n",
        "\n",
        "    def reset_grad(self):\n",
        "        \"\"\"Reset the gradient buffers.\"\"\"\n",
        "        self.g_optimizer.zero_grad()\n",
        "        self.d_optimizer.zero_grad()\n",
        "        self.c_optimizer.zero_grad()\n",
        "\n",
        "    def restore_model(self, resume_iters):\n",
        "        \"\"\"Restore the trained generator and discriminator.\"\"\"\n",
        "        print('Loading the trained models from step {}...'.format(resume_iters))\n",
        "        G_path = os.path.join(self.model_save_dir, '{}-G.ckpt'.format(resume_iters))\n",
        "        D_path = os.path.join(self.model_save_dir, '{}-D.ckpt'.format(resume_iters))\n",
        "        C_path = os.path.join(self.model_save_dir, '{}-C.ckpt'.format(resume_iters))\n",
        "        self.G.load_state_dict(torch.load(G_path, map_location=lambda storage, loc: storage))\n",
        "        self.D.load_state_dict(torch.load(D_path, map_location=lambda storage, loc: storage))\n",
        "        self.C.load_state_dict(torch.load(C_path, map_location=lambda storage, loc: storage))\n",
        "\n",
        "    @staticmethod\n",
        "    def pad_coded_sp(coded_sp_norm):\n",
        "        f_len = coded_sp_norm.shape[1]\n",
        "        if  f_len >= FRAMES: \n",
        "            pad_length = FRAMES-(f_len - (f_len//FRAMES) * FRAMES)\n",
        "        elif f_len < FRAMES:\n",
        "            pad_length = FRAMES - f_len\n",
        "\n",
        "        sp_norm_pad = np.hstack((coded_sp_norm, np.zeros((coded_sp_norm.shape[0], pad_length))))\n",
        "        return sp_norm_pad \n",
        "\n",
        "    def test(self):\n",
        "        \"\"\"Translate speech using StarGAN .\"\"\"\n",
        "        # Load the trained generator.\n",
        "        self.restore_model(self.test_iters)\n",
        "        norm = Normalizer()\n",
        "\n",
        "        # Set data loader.\n",
        "        d, speaker = TestSet(self.test_dir).test_data(self.src_speaker)\n",
        "        targets = self.trg_speaker\n",
        "       \n",
        "        for target in targets:\n",
        "            print(target)\n",
        "            assert target in speakers\n",
        "            label_t = self.spk_enc.transform([target])[0]\n",
        "            label_t = np.asarray([label_t])\n",
        "            \n",
        "            with torch.no_grad():\n",
        "\n",
        "                for filename, content in d.items():\n",
        "                    f0 = content['f0']\n",
        "                    ap = content['ap']\n",
        "                    sp_norm_pad = self.pad_coded_sp(content['coded_sp_norm'])\n",
        "\n",
        "                    convert_result = []\n",
        "                    for start_idx in range(0, sp_norm_pad.shape[1] - FRAMES + 1, FRAMES):\n",
        "                        one_seg = sp_norm_pad[:, start_idx : start_idx+FRAMES]\n",
        "                        \n",
        "                        one_seg = torch.FloatTensor(one_seg).to(self.device)\n",
        "                        one_seg = one_seg.view(1,1,one_seg.size(0), one_seg.size(1))\n",
        "                        l = torch.FloatTensor(label_t)\n",
        "                        one_seg = one_seg.to(self.device)\n",
        "                        l = l.to(self.device)\n",
        "                        one_set_return = self.G(one_seg, l).data.cpu().numpy()\n",
        "                        one_set_return = np.squeeze(one_set_return)\n",
        "                        one_set_return = norm.backward_process(one_set_return, target)\n",
        "                        convert_result.append(one_set_return)\n",
        "\n",
        "                    convert_con = np.concatenate(convert_result, axis=1)\n",
        "                    convert_con = convert_con[:, 0:content['coded_sp_norm'].shape[1]]\n",
        "                    contigu = np.ascontiguousarray(convert_con.T, dtype=np.float64)   \n",
        "                    decoded_sp = decode_spectral_envelope(contigu, SAMPLE_RATE, fft_size=FFTSIZE)\n",
        "                    f0_converted = norm.pitch_conversion(f0, speaker, target)\n",
        "                    wav = synthesize(f0_converted, decoded_sp, ap, SAMPLE_RATE)\n",
        "\n",
        "                    name = f'{speaker}-{target}_iter{self.test_iters}_{filename}'\n",
        "                    path = os.path.join(self.result_dir, name)\n",
        "                    print(f'[save]:{path}')\n",
        "                    #librosa.output.write_wav(path, wav, SAMPLE_RATE)\n",
        "                    \n",
        "                    sf.write(path, wav, SAMPLE_RATE)            \n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "if __name__ == '__main__':\n",
        "    pass"
      ],
      "metadata": {
        "id": "WPGJNH4304UW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import argparse\n",
        "#from solver import Solver\n",
        "#from data_loader import data_loader\n",
        "from torch.backends import cudnn\n",
        "\n",
        "\n",
        "def str2bool(v):\n",
        "    return v.lower() in ('true')\n",
        "\n",
        "\n",
        "def main(config):\n",
        "    # For fast training.\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "    # Create directories if not exist.\n",
        "    if not os.path.exists(config.log_dir):\n",
        "        os.makedirs(config.log_dir)\n",
        "    if not os.path.exists(config.model_save_dir):\n",
        "        os.makedirs(config.model_save_dir)\n",
        "    if not os.path.exists(config.sample_dir):\n",
        "        os.makedirs(config.sample_dir)\n",
        "    if not os.path.exists(config.result_dir):\n",
        "        os.makedirs(config.result_dir)\n",
        "\n",
        "    # Data loader.\n",
        "    \n",
        "    dloader = data_loader(config.data_dir, batch_size=config.batch_size, mode=config.mode,num_workers=config.num_workers)\n",
        "\n",
        "    # Solver for training and testing StarGAN.\n",
        "    solver = Solver(dloader, config)\n",
        "\n",
        "    if config.mode == 'train':\n",
        "        solver.train()\n",
        "\n",
        "    elif config.mode == 'test':\n",
        "        solver.test()\n",
        "      \n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # Model configuration.\n",
        "\n",
        "    parser.add_argument('--lambda_cycle', type=float, default=3, help='weight for cycle loss')\n",
        "    parser.add_argument('--lambda_cls', type=float, default=2, help='weight for domain classification loss')\n",
        "    \n",
        "    parser.add_argument('--lambda_identity', type=float, default=2, help='weight for identity loss')\n",
        "    \n",
        "    # Training configuration.\n",
        "    \n",
        "    parser.add_argument('--batch_size', type=int, default=4, help='mini-batch size')\n",
        "    parser.add_argument('--num_iters', type=int, default=10, help='number of total iterations for training D')\n",
        "    parser.add_argument('--num_iters_decay', type=int, default=5, help='number of iterations for decaying lr')\n",
        "    parser.add_argument('--g_lr', type=float, default=0.0001, help='learning rate for G')\n",
        "    parser.add_argument('--d_lr', type=float, default=0.0001, help='learning rate for D')\n",
        "    parser.add_argument('--c_lr', type=float, default=0.0001, help='learning rate for C')\n",
        "    parser.add_argument('--n_critic', type=int, default=5, help='number of D updates per each G update')\n",
        "    parser.add_argument('--beta1', type=float, default=0.5, help='beta1 for Adam optimizer')\n",
        "    parser.add_argument('--beta2', type=float, default=0.999, help='beta2 for Adam optimizer')\n",
        "    parser.add_argument('--resume_iters', type=int, default=None, help='resume training from this step')\n",
        "    \n",
        "    \n",
        "\n",
        "    # Test configuration.\n",
        "    parser.add_argument('--test_iters', type=int, default=10, help='test model from this step')\n",
        "    parser.add_argument('--src_speaker', type=str, default=None, help='test model source speaker')\n",
        "    parser.add_argument('--trg_speaker', type=str, default=\"['SF1', 'TM1']\", help='string list repre of target speakers eg.\"[a,b]\"')\n",
        "\n",
        "    # Miscellaneous.\n",
        "    parser.add_argument('--num_workers', type=int, default=4)\n",
        "    parser.add_argument('--mode', type=str, default='train', choices=['train', 'test'])\n",
        "    parser.add_argument('--use_tensorboard', type=str2bool, default=True)\n",
        "\n",
        "    # Directories.\n",
        "    parser.add_argument('--data_dir', type=str, default='data/processed')\n",
        "    parser.add_argument('--test_dir', type=str, default='data/speakers_test')\n",
        "    parser.add_argument('--log_dir', type=str, default='starganvc/logs')\n",
        "    parser.add_argument('--model_save_dir', type=str, default='starganvc/models')\n",
        "    parser.add_argument('--sample_dir', type=str, default='starganvc/samples')\n",
        "    parser.add_argument('--result_dir', type=str, default='starganvc/results')\n",
        "    \n",
        "    # Step size.\n",
        "    parser.add_argument('--log_step', type=int, default=10)\n",
        "    parser.add_argument('--sample_step', type=int, default=2000)\n",
        "    parser.add_argument('--model_save_step', type=int, default=10)\n",
        "    parser.add_argument('--lr_update_step', type=int, default=5)\n",
        "\n",
        "    config = parser.parse_args(\"\")\n",
        "    print(config)\n",
        "    main(config)\n",
        "    "
      ],
      "metadata": {
        "id": "bakpwab406Hl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import argparse\n",
        "#from solver import Solver\n",
        "#from data_loader import data_loader\n",
        "from torch.backends import cudnn\n",
        "\n",
        "\n",
        "def str2bool(v):\n",
        "    return v.lower() in ('true')\n",
        "\n",
        "\n",
        "def main(config):\n",
        "    # For fast training.\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "    # Create directories if not exist.\n",
        "    if not os.path.exists(config.log_dir):\n",
        "        os.makedirs(config.log_dir)\n",
        "    if not os.path.exists(config.model_save_dir):\n",
        "        os.makedirs(config.model_save_dir)\n",
        "    if not os.path.exists(config.sample_dir):\n",
        "        os.makedirs(config.sample_dir)\n",
        "    if not os.path.exists(config.result_dir):\n",
        "        os.makedirs(config.result_dir)\n",
        "\n",
        "    # Data loader.\n",
        "    \n",
        "    dloader = data_loader(config.data_dir, batch_size=config.batch_size, mode=config.mode,num_workers=config.num_workers)\n",
        "\n",
        "    # Solver for training and testing StarGAN.\n",
        "    solver = Solver(dloader, config)\n",
        "\n",
        "    if config.mode == 'train':\n",
        "        solver.train()\n",
        "\n",
        "    elif config.mode == 'test':\n",
        "        solver.test()\n",
        "#        torch.save(solver.state_dict(), \"solver.pth\")\n",
        " #       print(\"Saved PyTorch Model State to solver.pth\") \n",
        "      \n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # Model configuration.\n",
        "\n",
        "    parser.add_argument('--lambda_cycle', type=float, default=3, help='weight for cycle loss')\n",
        "    parser.add_argument('--lambda_cls', type=float, default=2, help='weight for domain classification loss')\n",
        "    \n",
        "    parser.add_argument('--lambda_identity', type=float, default=2, help='weight for identity loss')\n",
        "    \n",
        "    # Training configuration.\n",
        "    \n",
        "    parser.add_argument('--batch_size', type=int, default=4, help='mini-batch size')\n",
        "    parser.add_argument('--num_iters', type=int, default=10, help='number of total iterations for training D')\n",
        "    parser.add_argument('--num_iters_decay', type=int, default=5, help='number of iterations for decaying lr')\n",
        "    parser.add_argument('--g_lr', type=float, default=0.0001, help='learning rate for G')\n",
        "    parser.add_argument('--d_lr', type=float, default=0.0001, help='learning rate for D')\n",
        "    parser.add_argument('--c_lr', type=float, default=0.0001, help='learning rate for C')\n",
        "    parser.add_argument('--n_critic', type=int, default=5, help='number of D updates per each G update')\n",
        "    parser.add_argument('--beta1', type=float, default=0.5, help='beta1 for Adam optimizer')\n",
        "    parser.add_argument('--beta2', type=float, default=0.999, help='beta2 for Adam optimizer')\n",
        "    parser.add_argument('--resume_iters', type=int, default=None, help='resume training from this step')\n",
        "    \n",
        "    \n",
        "\n",
        "    # Test configuration.\n",
        "    parser.add_argument('--test_iters', type=int, default=10, help='test model from this step')\n",
        "    parser.add_argument('--src_speaker', type=str, default=\"TM1\", help='test model source speaker')\n",
        "    parser.add_argument('--trg_speaker', type=str, default=\"['TM2', 'SF1']\", help='string list repre of target speakers eg.\"[a,b]\"')\n",
        "\n",
        "    # Miscellaneous.\n",
        "    parser.add_argument('--num_workers', type=int, default=4)\n",
        "    parser.add_argument('--mode', type=str, default='test', choices=['train', 'test'])\n",
        "    parser.add_argument('--use_tensorboard', type=str2bool, default=True)\n",
        "\n",
        "    # Directories.\n",
        "    parser.add_argument('--data_dir', type=str, default='data/processed')\n",
        "    parser.add_argument('--test_dir', type=str, default='data/speakers_test')\n",
        "    parser.add_argument('--log_dir', type=str, default='starganvc/logs')\n",
        "    parser.add_argument('--model_save_dir', type=str, default='starganvc/models')\n",
        "    parser.add_argument('--sample_dir', type=str, default='starganvc/samples')\n",
        "    parser.add_argument('--result_dir', type=str, default='starganvc/results')\n",
        "    \n",
        "    # Step size.\n",
        "    parser.add_argument('--log_step', type=int, default=10)\n",
        "    parser.add_argument('--sample_step', type=int, default=2000)\n",
        "    parser.add_argument('--model_save_step', type=int, default=10)\n",
        "    parser.add_argument('--lr_update_step', type=int, default=5)\n",
        "\n",
        "    config = parser.parse_args(\"\")\n",
        "    print(config)\n",
        "    main(config)\n",
        "    "
      ],
      "metadata": {
        "id": "w5QfIccG1Cvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x36v_ymQ1Vso"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}